<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>用chatgpt学习nlp（一） | Lyon个人博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="用chatgpt学习新知识是一件很有趣的事情，交互感特别强，以下是我尝试用chatgpt学习nlp的示例，其中去掉了一些无效的提问和回答。">
<meta property="og:type" content="article">
<meta property="og:title" content="用chatgpt学习nlp（一）">
<meta property="og:url" content="http://example.com/2023/04/11/%E7%94%A8chatgpt%E5%AD%A6%E4%B9%A0nlp%EF%BC%88%E4%B8%80%EF%BC%89/index.html">
<meta property="og:site_name" content="Lyon个人博客">
<meta property="og:description" content="用chatgpt学习新知识是一件很有趣的事情，交互感特别强，以下是我尝试用chatgpt学习nlp的示例，其中去掉了一些无效的提问和回答。">
<meta property="og:locale">
<meta property="article:published_time" content="2023-04-11T08:37:39.000Z">
<meta property="article:modified_time" content="2023-04-11T08:48:32.016Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Lyon个人博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Lyon个人博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-用chatgpt学习nlp（一）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/04/11/%E7%94%A8chatgpt%E5%AD%A6%E4%B9%A0nlp%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-date">
  <time datetime="2023-04-11T08:37:39.000Z" itemprop="datePublished">2023-04-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      用chatgpt学习nlp（一）
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>用chatgpt学习新知识是一件很有趣的事情，交互感特别强，以下是我尝试用chatgpt学习nlp的示例，其中去掉了一些无效的提问和回答。</p>
<span id="more"></span>

<blockquote>
<p>问： can you teach me nlp</p>
</blockquote>
<p>答：</p>
<p>Yes, I can definitely help you learn NLP! However, NLP is a complex field with many different sub-disciplines, so it would be helpful to know what specific areas of NLP you are interested in learning about. Here are some common topics in NLP that I can teach you:</p>
<ol>
<li>Text preprocessing: This includes techniques like tokenization, stemming, and stopword removal that are used to clean and transform text data.</li>
<li>Text classification: This involves classifying text data into different categories or classes based on its content. Examples of text classification tasks include spam detection, sentiment analysis, and topic modeling.</li>
<li>Named Entity Recognition (NER): This involves identifying and classifying named entities in text data, such as people, places, and organizations.</li>
<li>Part-of-Speech (POS) tagging: This involves identifying the part of speech for each word in a sentence, such as noun, verb, or adjective.</li>
<li>Parsing: This involves analyzing the structure of a sentence to identify its grammatical components, such as subjects, verbs, and objects.</li>
<li>Machine translation: This involves automatically translating text from one language to another.</li>
<li>Question answering: This involves building systems that can automatically answer questions posed in natural language.</li>
</ol>
<p>Let me know which of these topics interests you the most, and I can provide more guidance on how to get started!</p>
<blockquote>
<p>问： Text preprocessing</p>
</blockquote>
<p>答：</p>
<p>Great, let’s start with text preprocessing. Text preprocessing involves transforming raw text data into a format that can be easily processed by NLP algorithms. Here are some common techniques used in text preprocessing:</p>
<ol>
<li>Tokenization: This involves splitting the text into individual words or tokens. Tokenization can be done using whitespace, punctuation, or other criteria, depending on the specific task.</li>
<li>Stopword removal: This involves removing common words like “the,” “and,” and “or” from the text data, as they do not carry much meaning and can negatively impact the accuracy of NLP models.</li>
<li>Stemming and lemmatization: These are techniques for reducing words to their base or root form. For example, “running,” “runs,” and “ran” would all be reduced to “run” using stemming or lemmatization.</li>
<li>Lowercasing: This involves converting all words to lowercase, as NLP models often treat uppercase and lowercase words differently.</li>
<li>Spell correction: This involves correcting common spelling errors in the text data, such as replacing “teh” with “the.”</li>
</ol>
<p>Here’s an example code snippet in Python that demonstrates how to perform some of these text preprocessing techniques using the Natural Language Toolkit (NLTK) library:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load NLTK resources</span></span><br><span class="line">nltk.download(<span class="string">&#x27;punkt&#x27;</span>)</span><br><span class="line">nltk.download(<span class="string">&#x27;stopwords&#x27;</span>)</span><br><span class="line">nltk.download(<span class="string">&#x27;wordnet&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample text data</span></span><br><span class="line">text = <span class="string">&quot;The quick brown fox jumped over the lazy dog.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenization</span></span><br><span class="line">tokens = word_tokenize(text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Stopword removal</span></span><br><span class="line">stop_words = <span class="built_in">set</span>(stopwords.words(<span class="string">&#x27;english&#x27;</span>))</span><br><span class="line">filtered_tokens = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens <span class="keyword">if</span> token.lower() <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Lemmatization</span></span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line">lemmatized_tokens = [lemmatizer.lemmatize(token) <span class="keyword">for</span> token <span class="keyword">in</span> filtered_tokens]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lemmatized_tokens)</span><br></pre></td></tr></table></figure>

<p>This code loads the NLTK library and downloads the necessary resources. It then performs tokenization, stopword removal, and lemmatization on the sample text data, and prints the resulting lemmatized tokens. Note that you will need to install the NLTK library to run this code.</p>
<blockquote>
<p>问： what does it print</p>
</blockquote>
<p>答：</p>
<p>The code snippet I provided prints the resulting list of lemmatized tokens after performing text preprocessing on the sample text data. Specifically, it performs tokenization, stopword removal, and lemmatization on the text data, and then prints the resulting list of lemmatized tokens.</p>
<p>For the sample text data “The quick brown fox jumped over the lazy dog,” the code will print the following list of lemmatized tokens:</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[<span class="string">&#x27;quick&#x27;</span>, <span class="string">&#x27;brown&#x27;</span>, <span class="string">&#x27;fox&#x27;</span>, <span class="string">&#x27;jumped&#x27;</span>, <span class="string">&#x27;lazy&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span></span><br></pre></td></tr></table></figure>

<p>Note that the text has been tokenized into individual words, stop words like “the” and “over” have been removed, and words like “jumped” and “lazy” have been lemmatized to their base form.</p>
<blockquote>
<p>问：  What’s the use of this</p>
</blockquote>
<p>答：</p>
<p>Text preprocessing is an important step in NLP because it helps to transform raw text data into a format that can be easily processed by NLP algorithms. By applying techniques like tokenization, stopword removal, and lemmatization, we can reduce the dimensionality of the text data and remove noise and irrelevant information, which can improve the accuracy of NLP models.</p>
<p>In the example code snippet I provided, we took a sample text data and performed tokenization, stopword removal, and lemmatization on it. The resulting lemmatized tokens can be used as input to further NLP tasks, such as text classification or sentiment analysis. For example, we could use these lemmatized tokens as input to a machine learning model to classify the text as positive or negative, or to extract important information like named entities from the text.</p>
<p>Overall, text preprocessing is an essential step in NLP that helps to make text data more manageable and relevant for downstream NLP tasks.</p>
<blockquote>
<p>问： The resulting lemmatized tokens can be used as input to further NLP tasks. give me a sample about this</p>
</blockquote>
<p>答：</p>
<p>Sure, here’s an example of how we can use the lemmatized tokens generated by the text preprocessing step as input to a text classification task using a machine learning model.</p>
<p>Let’s say we want to classify the sample text “The quick brown fox jumped over the lazy dog” into one of two categories: “animal” or “not animal.” To do this, we can use a bag-of-words approach, where we represent the text as a vector of word counts, and train a machine learning model on this representation.</p>
<p>Here’s some example code that demonstrates how to perform text classification using the lemmatized tokens generated by the text preprocessing step:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample training data</span></span><br><span class="line">train_texts = [</span><br><span class="line">    <span class="string">&quot;The quick brown fox jumped over the lazy dog&quot;</span>,</span><br><span class="line">    <span class="string">&quot;I saw a red car on the highway&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The cat in the hat is a children&#x27;s book&quot;</span></span><br><span class="line">]</span><br><span class="line">train_labels = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>] <span class="comment"># 1 = animal, 0 = not animal</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Text preprocessing</span></span><br><span class="line">lemmatized_texts = []</span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> train_texts:</span><br><span class="line">    tokens = word_tokenize(text)</span><br><span class="line">    filtered_tokens = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens <span class="keyword">if</span> token.lower() <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line">    lemmatized_tokens = [lemmatizer.lemmatize(token) <span class="keyword">for</span> token <span class="keyword">in</span> filtered_tokens]</span><br><span class="line">    lemmatized_texts.append(<span class="string">&quot; &quot;</span>.join(lemmatized_tokens))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bag-of-words representation</span></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">X_train = vectorizer.fit_transform(lemmatized_texts)</span><br><span class="line">y_train = np.array(train_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train a naive Bayes classifier</span></span><br><span class="line">clf = MultinomialNB()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample test data</span></span><br><span class="line">test_texts = [</span><br><span class="line">    <span class="string">&quot;The lazy dog slept all day&quot;</span>,</span><br><span class="line">    <span class="string">&quot;I ate a sandwich for lunch&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Text preprocessing</span></span><br><span class="line">lemmatized_test_texts = []</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> test_texts:</span><br><span class="line">    tokens = word_tokenize(text)</span><br><span class="line">    filtered_tokens = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens <span class="keyword">if</span> token.lower() <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line">    lemmatized_tokens = [lemmatizer.lemmatize(token) <span class="keyword">for</span> token <span class="keyword">in</span> filtered_tokens]</span><br><span class="line">    lemmatized_test_texts.append(<span class="string">&quot; &quot;</span>.join(lemmatized_tokens))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bag-of-words representation</span></span><br><span class="line">X_test = vectorizer.transform(lemmatized_test_texts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict on test data</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure>

<p>In this code, we start by defining a sample training dataset consisting of three text strings and their corresponding labels. We then perform text preprocessing on the text data using tokenization, stopword removal, and lemmatization, and represent the resulting lemmatized tokens as a bag-of-words using the CountVectorizer class from scikit-learn.</p>
<p>We then train a Naive Bayes classifier on the bag-of-words representation of the training data using the MultinomialNB class from scikit-learn. Finally, we define some sample test data, perform text preprocessing on it, represent it as a bag-of-words using the same vectorizer as the training data, and predict the labels using the trained classifier.</p>
<p>This code will output a list of predicted labels for the test data. For example, if the test data is “The lazy dog slept all day” and “I ate a sandwich for lunch,” the output might be <code>[1, 0]</code>, indicating that the first text is classified as an animal and the second text is classified as not an animal.</p>
<p>Note that this is just one example of how we can use text preprocessing and bag-of-words representations to perform NLP tasks, and there are many other techniques and models available depending on the specific task at hand.</p>
<blockquote>
<p>问： explain bag-of-words</p>
</blockquote>
<p>答：</p>
<p>Bag-of-words is a commonly used method in natural language processing (NLP) for representing text data as numerical vectors that can be used as input to machine learning models.</p>
<p>In a bag-of-words representation, we first tokenize the text data by splitting it into individual words or tokens, and then we count the number of occurrences of each word in the text. We then represent the text as a vector of these word counts, where each element in the vector corresponds to a particular word in the vocabulary and its value is the count of that word in the text.</p>
<p>For example, consider the sentence “The quick brown fox jumps over the lazy dog.” A bag-of-words representation of this sentence might look like:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">| Word | Count |</span><br><span class="line">| --- | --- |</span><br><span class="line">| the | 2 |</span><br><span class="line">| quick | 1 |</span><br><span class="line">| brown | 1 |</span><br><span class="line">| fox | 1 |</span><br><span class="line">| jumps | 1 |</span><br><span class="line">| over | 1 |</span><br><span class="line">| lazy | 1 |</span><br><span class="line">| dog | 1 |</span><br></pre></td></tr></table></figure>

<p>We can represent this bag-of-words as a vector with length equal to the size of the vocabulary, and each element in the vector corresponds to the count of a particular word in the text. For example, if our vocabulary consists of the words “the”, “quick”, “brown”, “fox”, “jumps”, “over”, “lazy”, and “dog”, the bag-of-words vector for the sentence above would be:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2, 1, 1, 1, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure>

<p>Bag-of-words representations have some limitations, as they do not take into account the order or context of the words in the text. However, they are still useful for many NLP tasks such as sentiment analysis, topic modeling, and text classification.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/04/11/%E7%94%A8chatgpt%E5%AD%A6%E4%B9%A0nlp%EF%BC%88%E4%B8%80%EF%BC%89/" data-id="clgc36ki7000c3wwg55dn266l" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/04/20/xxl-job%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0-%E6%B3%A8%E5%86%8C/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          xxl-job源码分析笔记（一）-注册
        
      </div>
    </a>
  
  
    <a href="/2023/03/31/xv6-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E5%AE%9E%E7%8E%B0%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">xv6 操作系统 实现系统调用</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ThreadLocal/" rel="tag">ThreadLocal</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jvm/" rel="tag">jvm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rpc/" rel="tag">rpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/xv6/" rel="tag">xv6</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/xxl-job/" rel="tag">xxl-job</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ThreadLocal/" style="font-size: 10px;">ThreadLocal</a> <a href="/tags/jvm/" style="font-size: 20px;">jvm</a> <a href="/tags/rpc/" style="font-size: 15px;">rpc</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/xxl-job/" style="font-size: 10px;">xxl-job</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/06/02/TransmittableThreadLocal%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">TransmittableThreadLocal实现原理</a>
          </li>
        
          <li>
            <a href="/2023/06/02/InheritableThreadLocal%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">InheritableThreadLocal实现原理</a>
          </li>
        
          <li>
            <a href="/2023/06/01/TheadLocal%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/">TheadLocal内存泄漏及原理分析</a>
          </li>
        
          <li>
            <a href="/2023/06/01/http-client-%E8%BF%9E%E6%8E%A5%E6%B1%A0%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">http client 连接池源码分析</a>
          </li>
        
          <li>
            <a href="/2023/04/23/xxl-job%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E6%89%A7%E8%A1%8C%E5%99%A8%E5%BC%82%E6%AD%A5%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1%E5%B9%B6%E5%9B%9E%E8%B0%83%E8%B0%83%E5%BA%A6%E4%B8%AD%E5%BF%83/">xxl-job源码分析笔记（四）执行器异步执行任务并回调调度中心</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>